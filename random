series_meta = gilt_df.drop_duplicates(["series_id","maturity_year"])
def pick_series(row):
    cands = series_meta[series_meta["maturity_year"]==row["maturity_year"]]
    return cands.iloc[0]["series_id"] if not cands.empty else None
calendar_df["series_id"] = calendar_df.apply(pick_series, axis=1)
cal_matched = calendar_df.dropna(subset=["series_id"])


panel = []
for _,a in cal_matched.iterrows():
    g = gilt_df[gilt_df["series_id"]==a["series_id"]].copy()
    g = g.sort_values("date")
    dates = list(g["date"].unique())
    if a["auction_dt"] not in dates: continue
    pos = dates.index(a["auction_dt"])
    for k,d in enumerate(dates):
        rel = k-pos
        if -10 <= rel <= 10:
            y = g.loc[g["date"]==d,"yld"].mean()
            panel.append({
                "auction_dt": a["auction_dt"],
                "series_id": a["series_id"],
                "date": d,
                "rel_day": rel,
                "y_close": y,
                "bucket": a["bucket"]
            })
panel = pd.DataFrame(panel)




avg_yld = (panel.groupby(["bucket","rel_day"])["y_close"]
                 .mean().reset_index()
                 .pivot(index="rel_day", columns="bucket", values="y_close"))





def concessions(df):
    sub = df[df["rel_day"].between(-10,10)]
    grp = sub.groupby(["auction_dt","series_id"])
    t_5 = grp.apply(lambda g: g.loc[g["rel_day"]==-5,"y_close"].mean())
    t_1 = grp.apply(lambda g: g.loc[g["rel_day"]==-1,"y_close"].mean())
    t0  = grp.apply(lambda g: g.loc[g["rel_day"]==0,"y_close"].mean())
    p5  = grp.apply(lambda g: g.loc[g["rel_day"]==5,"y_close"].mean())
    return pd.DataFrame({
        "cons_pre_5d": t0 - t_5,
        "cons_pre_1d": t0 - t_1,
        "reversal_5d": p5 - t0
    }).reset_index()

cons_yld = concessions(panel)
bucket_map = panel.drop_duplicates(["auction_dt","series_id"])[["auction_dt","series_id","bucket"]]
summary = cons_yld.merge(bucket_map,on=["auction_dt","series_id"]).groupby("bucket").mean()


print("\nConcession summary by bucket (levels are in percentage points; ×100 for bp):")
print(summary.round(4))

# Optional: also show distribution per bucket
dist = (per_auction.merge(bucket_map, on=["auction_dt","series_id"], how="left")
                  .groupby("bucket")[["cons_pre_5d","cons_pre_1d","reversal_5d"]]
                  .describe())







import re
from datetime import date
from pathlib import Path
from typing import Optional, Tuple, Union

import pandas as pd

# -------------------------
# 1) Helpers: parsing & normalisation
# -------------------------

# Map common unicode fraction symbols to floats (e.g. "4½%" -> 4.5)
_FRACTION_MAP = {
    "¼": 0.25, "½": 0.5, "¾": 0.75,
    "⅐": 1/7, "⅑": 1/9, "⅒": 0.1,
    "⅓": 1/3, "⅔": 2/3, "⅕": 0.2, "⅖": 0.4, "⅗": 0.6, "⅘": 0.8,
    "⅙": 1/6, "⅚": 5/6, "⅛": 0.125, "⅜": 0.375, "⅝": 0.625, "⅞": 0.875,
}

def _normalise_coupon_str(s: str) -> float:
    """
    Accepts things like '4½%' or '0.625%' or '4.5%'.
    Returns float coupon in percent (e.g. 4.5).
    """
    s = s.replace("%", "").strip()
    # If s includes unicode fraction, split integer + fraction
    m = re.match(r"^\s*(\d+)?\s*([¼½¾⅐⅑⅒⅓⅔⅕⅖⅗⅘⅙⅚⅛⅜⅝⅞])?\s*$", s)
    if m and (m.group(1) or m.group(2)):
        whole = float(m.group(1)) if m.group(1) else 0.0
        frac = _FRACTION_MAP.get(m.group(2), 0.0)
        return whole + frac
    # Else parse regular float
    return float(s)

def parse_auction_security(name: str) -> Tuple[Optional[float], Optional[date], Optional[int]]:
    """
    Parse coupon, maturity_date (if present), and maturity_year from an auction security_name.
    Works for strings like:
      '4½% Treasury Gilt 2033'
      '0.125% Treasury Gilt 31-Jan-2031'
    Returns (coupon_pct, maturity_date, maturity_year)
    """
    if not isinstance(name, str):
        return None, None, None

    # 1) coupon: allow unicode fractions OR decimal before '%'
    m_coupon = re.search(r"([0-9]+(?:\.[0-9]+)?)%|([0-9\s¼½¾⅐⅑⅒⅓⅔⅕⅖⅗⅘⅙⅚⅛⅜⅝⅞]+)%", name)
    coupon = None
    if m_coupon:
        raw = m_coupon.group(1) or m_coupon.group(2)
        coupon = _normalise_coupon_str(raw)

    # 2) try full maturity date DD-MMM-YYYY
    m_date = re.search(r"(\d{2}-[A-Za-z]{3}-\d{4})", name)
    mty_date = None
    if m_date:
        try:
            mty_date = pd.to_datetime(m_date.group(1), format="%d-%b-%Y").date()
        except Exception:
            mty_date = None

    # 3) year (last 4-digit year in string)
    m_year = re.findall(r"(\d{4})", name)
    mty_year = int(m_year[-1]) if m_year else None

    return coupon, mty_date, mty_year

def parse_market_header(header: str) -> Tuple[Optional[float], Optional[date], Optional[int]]:
    """
    Parse coupon/maturity from the market timeseries header (2nd column name),
    e.g. 'UKT GBP 0.625 31-Jul-2035 LON | PX_MID | YLD'
    Returns (coupon_pct, maturity_date, maturity_year)
    """
    if not isinstance(header, str):
        return None, None, None
    # coupon: first float after 'UKT GBP ' or anywhere
    m_coupon = re.search(r"\b([0-9]+(?:\.[0-9]+)?)\b", header)
    coupon = float(m_coupon.group(1)) if m_coupon else None

    # full maturity date DD-MMM-YYYY
    m_date = re.search(r"(\d{2}-[A-Za-z]{3}-\d{4})", header)
    mty_date = None
    if m_date:
        try:
            mty_date = pd.to_datetime(m_date.group(1), format="%d-%b-%Y").date()
        except Exception:
            pass

    mty_year = mty_date.year if mty_date else None
    # fallback: find any 4-digit year
    if mty_year is None:
        m_year = re.search(r"\b(20\d{2}|19\d{2})\b", header)
        mty_year = int(m_year.group(1)) if m_year else None

    return coupon, mty_date, mty_year


# -------------------------
# 2) Loaders with rich metadata (series_id from B1)
# -------------------------

def load_gilt_file(path: Path) -> pd.DataFrame:
    """
    Load a CSV/XLS file of a single gilt series.
    Extract `series_id` from B1, and parse coupon/maturity from the yld column header.
    Returns tidy columns: date, series_id, coupon_pct, maturity_date, maturity_year, yld
    """
    path = Path(path)
    is_excel = path.suffix.lower() in (".xlsx", ".xls")

    if is_excel:
        df = pd.read_excel(path)
        series_id = pd.read_excel(path, header=None).iloc[0, 1]
    else:
        df = pd.read_csv(path)
        series_id = pd.read_csv(path, header=None).iloc[0, 1]

    # standardise date
    df = df.rename(columns={"Date": "date"})
    df["date"] = pd.to_datetime(df["date"], errors="coerce").dt.date

    # yield column assumed second
    yld_col = df.columns[1]

    # parse header info
    coupon, mty_date, mty_year = parse_market_header(str(yld_col))

    out = pd.DataFrame({
        "date": df["date"],
        "series_id": series_id,
        "coupon_pct": coupon,
        "maturity_date": mty_date,
        "maturity_year": mty_year,
        "yld": pd.to_numeric(df[yld_col], errors="coerce"),
    })
    return out.dropna(subset=["yld"])

def load_gilt_folder(folder: Union[str, Path]) -> pd.DataFrame:
    folder = Path(folder)
    frames = []
    for fn in folder.iterdir():
        if fn.suffix.lower() in (".csv", ".xlsx", ".xls"):
            frames.append(load_gilt_file(fn))
    if not frames:
        raise RuntimeError(f"No CSV/XLS files found in {folder}")
    return pd.concat(frames, ignore_index=True)


# -------------------------
# 3) Enrich auctions and exact-line matching
# -------------------------

def enrich_auction_calendar(calendar_df: pd.DataFrame) -> pd.DataFrame:
    """
    Adds coupon_pct, maturity_date (if present in name), and maturity_year to auction calendar.
    Assumes there is a 'security_name' and 'auction_dt' column.
    """
    parsed = calendar_df["security_name"].apply(parse_auction_security)
    calendar_df = calendar_df.copy()
    calendar_df[["coupon_pct", "maturity_date", "maturity_year_from_name"]] = pd.DataFrame(parsed.tolist(), index=calendar_df.index)

    # prefer explicit maturity_date if found; otherwise keep just year
    calendar_df["maturity_year"] = calendar_df["maturity_date"].apply(lambda d: d.year if pd.notna(d) else None)
    calendar_df["maturity_year"] = calendar_df["maturity_year"].fillna(calendar_df["maturity_year_from_name"]).astype("Int64")
    return calendar_df.drop(columns=["maturity_year_from_name"])

def _score_series_id(sid: str) -> int:
    """
    Tie-breaker preference:
      - prefer 'LON', 'MID', 'PX_MID' in the id string
      - then longer history
    """
    sid_u = str(sid).upper()
    score = 0
    for k, w in enumerate(["LON", "PX_MID", "MID"]):
        if w in sid_u:
            score += (k + 1) * 100
    return score

def match_series_exact(calendar_df: pd.DataFrame, gilt_df: pd.DataFrame) -> pd.DataFrame:
    """
    Attaches the correct series_id to each auction by:
      1) Exact match on (coupon_pct, maturity_date) if auction has maturity_date.
      2) Else match on (coupon_pct, maturity_year).
    Requires the chosen series to cover the auction_dt.
    If multiple candidates remain, picks the one with highest score (venue/mid) then longest history.
    """
    cal = enrich_auction_calendar(calendar_df)

    # build series metadata
    coverage = (gilt_df
                .groupby("series_id")
                .agg(first_date=("date", "min"), last_date=("date", "max"),
                     coupon_pct=("coupon_pct", "first"),
                     maturity_date=("maturity_date", "first"),
                     maturity_year=("maturity_year", "first"))
                .reset_index())

    def choose(row):
        # candidates: exact maturity date when present on auction
        if pd.notna(row["maturity_date"]):
            cands = coverage[
                (coverage["coupon_pct"] == row["coupon_pct"]) &
                (coverage["maturity_date"] == row["maturity_date"])
            ]
        else:
            cands = coverage[
                (coverage["coupon_pct"] == row["coupon_pct"]) &
                (coverage["maturity_year"] == row["maturity_year"])
            ]

        # must cover auction date
        if not cands.empty:
            cands = cands[(cands["first_date"] <= row["auction_dt"]) & (cands["last_date"] >= row["auction_dt"])]

        if cands.empty:
            return None

        # rank by preference score + history length
        cands = cands.assign(
            pref_score=cands["series_id"].map(_score_series_id),
            hist_len=(pd.to_datetime(cands["last_date"]) - pd.to_datetime(cands["first_date"]))
        ).sort_values(["pref_score", "hist_len"], ascending=False)
        return cands.iloc[0]["series_id"]

    cal = cal.copy()
    cal["series_id"] = cal.apply(choose, axis=1)
    cal_matched = cal.dropna(subset=["series_id"]).reset_index(drop=True)
    return cal_matched

# -------------------------
# 4) Example usage
# -------------------------
# gilt_df = load_gilt_folder(r"\\...path...\rates\gilts")
# calendar_df = ...  # your auction calendar before matching (must include 'auction_dt','security_name')
# cal_matched = match_series_exact(calendar_df, gilt_df)
# cal_matched.head()
